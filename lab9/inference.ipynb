{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download and load model and tokenizer from Huggingface\n",
    "def load_model(model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Download and load a model and tokenizer from Huggingface.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model to download from Huggingface\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to compute log likelihood of a sentence\n",
    "def compute_log_likelihood(model, tokenizer, sentence, return_token_likelihoods=False):\n",
    "    \"\"\"\n",
    "    Compute the log likelihood of a sentence using a language model.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer for the language model\n",
    "        sentence (str): Input sentence\n",
    "        return_token_likelihoods (bool): Whether to return token-level likelihoods\n",
    "        \n",
    "    Returns:\n",
    "        float: Log likelihood of the sentence\n",
    "        list (optional): Token-level log likelihoods\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get input and target IDs\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    # Calculate log likelihood\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "        \n",
    "        # Get the loss - this is the negative log likelihood\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "        log_likelihood = -neg_log_likelihood\n",
    "        \n",
    "        if return_token_likelihoods:\n",
    "            # Get token-level log probabilities\n",
    "            logits = outputs.logits[:, :-1, :]  # Remove last position\n",
    "            shift_target_ids = target_ids[:, 1:]  # Shift to match prediction positions\n",
    "            \n",
    "            # Get log probabilities for each position\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Extract log probability of each target token\n",
    "            token_log_probs = []\n",
    "            for i in range(shift_target_ids.size(1)):\n",
    "                token_id = shift_target_ids[0, i].item()\n",
    "                token_log_prob = log_probs[0, i, token_id].item()\n",
    "                token = tokenizer.decode([token_id])\n",
    "                token_log_probs.append((token, token_log_prob))\n",
    "            \n",
    "            return log_likelihood, token_log_probs\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "# Load and evaluate BLiMP dataset\n",
    "def evaluate_blimp_paradigm(model, tokenizer, paradigm_name, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a BLiMP paradigm.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        paradigm_name (str): Name of the BLiMP paradigm\n",
    "        num_samples (int, optional): Number of samples to use, None for all\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including accuracy and score distributions\n",
    "    \"\"\"\n",
    "    # Load the specific BLiMP paradigm from Huggingface datasets\n",
    "    dataset = load_dataset(\"nyu-mll/blimp\", paradigm_name)\n",
    "    \n",
    "    # Extract sentence pairs\n",
    "    sentence_pairs = [(item['sentence_good'], item['sentence_bad']) \n",
    "                      for item in dataset['train']]\n",
    "    \n",
    "    if num_samples is not None:\n",
    "        sentence_pairs = sentence_pairs[:num_samples]\n",
    "    \n",
    "    results = {\n",
    "        'good_scores': [],\n",
    "        'bad_scores': [],\n",
    "        'score_diffs': [],\n",
    "        'correct_predictions': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"Evaluating {len(sentence_pairs)} sentence pairs...\")\n",
    "    \n",
    "    for good_sentence, bad_sentence in tqdm(sentence_pairs):\n",
    "        # Compute log likelihoods\n",
    "        good_ll = compute_log_likelihood(model, tokenizer, good_sentence)\n",
    "        bad_ll = compute_log_likelihood(model, tokenizer, bad_sentence)\n",
    "        \n",
    "        # Store results\n",
    "        results['good_scores'].append(good_ll)\n",
    "        results['bad_scores'].append(bad_ll)\n",
    "        results['score_diffs'].append(good_ll - bad_ll)\n",
    "        \n",
    "        # Correct prediction if good sentence has higher probability\n",
    "        if good_ll > bad_ll:\n",
    "            results['correct_predictions'] += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    results['accuracy'] = results['correct_predictions'] / len(sentence_pairs)\n",
    "    print(f\"Accuracy: {results['accuracy'] * 100:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze a specific sentence pair in detail\n",
    "def analyze_sentence_pair(model, tokenizer, good_sentence, bad_sentence):\n",
    "    \"\"\"\n",
    "    Analyze a specific sentence pair in detail.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        good_sentence (str): Grammatical sentence\n",
    "        bad_sentence (str): Ungrammatical sentence\n",
    "    \"\"\"\n",
    "    print(\"\\nDetailed Analysis of Sentence Pair:\")\n",
    "    print(f\"Grammatical:   '{good_sentence}'\")\n",
    "    print(f\"Ungrammatical: '{bad_sentence}'\")\n",
    "    \n",
    "    # Compute token-level log likelihoods\n",
    "    good_ll, good_token_lls = compute_log_likelihood(model, tokenizer, good_sentence, return_token_likelihoods=True)\n",
    "    bad_ll, bad_token_lls = compute_log_likelihood(model, tokenizer, bad_sentence, return_token_likelihoods=True)\n",
    "    \n",
    "    print(f\"\\nOverall Log Likelihood:\")\n",
    "    print(f\"Grammatical:   {good_ll:.4f}\")\n",
    "    print(f\"Ungrammatical: {bad_ll:.4f}\")\n",
    "    print(f\"Difference:    {good_ll - bad_ll:.4f}\")\n",
    "    \n",
    "    print(\"\\nToken-level Analysis:\")\n",
    "    print(\"Grammatical sentence token log probabilities:\")\n",
    "    for token, ll in good_token_lls:\n",
    "        print(f\"  '{token}': {ll:.4f}\")\n",
    "    \n",
    "    print(\"\\nUngrammatical sentence token log probabilities:\")\n",
    "    for token, ll in bad_token_lls:\n",
    "        print(f\"  '{token}': {ll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model(\"gpt2\")  # You can change to other models like \"gpt2-medium\", \"EleutherAI/gpt-neo-1.3B\", etc.\n",
    "\n",
    "# Example with a specific BLiMP paradigm\n",
    "# For more paradigms, see: https://huggingface.co/datasets/nyu-mll/blimp\n",
    "paradigm_name = \"island_effects\"\n",
    "\n",
    "# Evaluate the paradigm\n",
    "results = evaluate_blimp_paradigm(model, tokenizer, paradigm_name, num_samples=100)\n",
    "\n",
    "# Visualize results\n",
    "visualize_results(results, paradigm_name)\n",
    "\n",
    "# Get a sample pair from the dataset for detailed analysis\n",
    "dataset = load_dataset(\"nyu-mll/blimp\", paradigm_name)\n",
    "sample = dataset['train'][0]\n",
    "good_sentence = sample['sentence_good']\n",
    "bad_sentence = sample['sentence_bad']\n",
    "\n",
    "# Analyze the sample pair\n",
    "analyze_sentence_pair(model, tokenizer, good_sentence, bad_sentence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
